import os
from huggingface_hub import snapshot_download
from transformers import AutoTokenizer


model_name = "Qwen/Qwen3-4B"

# ensure model/tokenizer cache is on the pod (e.g., Runpod volume)
preferred_cache_dirs = [
    os.environ.get("HF_HOME"),
    "/runpod-volume/hf-cache" if os.path.isdir("/runpod-volume") else None,
    "/workspace/.cache/huggingface",
]
cache_dir = next((p for p in preferred_cache_dirs if p), "/workspace/.cache/huggingface")
os.makedirs(cache_dir, exist_ok=True)
os.environ.setdefault("HF_HOME", cache_dir)
os.environ.setdefault("HF_HUB_CACHE", os.path.join(cache_dir, "hub"))
os.environ.setdefault("TRANSFORMERS_CACHE", os.path.join(cache_dir, "transformers"))


def main():
    # Download tokenizer assets
    AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)

    # Download full model repository to cache without loading into GPU/CPU memory
    local_path = snapshot_download(repo_id=model_name, cache_dir=cache_dir, resume_download=True)
    print(f"Model snapshot downloaded to: {local_path}")


if __name__ == "__main__":
    main()
